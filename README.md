# Resources for Parameter-Efficient Fine-Tuning (PEFT)

## Must-Read Papers

<details><summary> <strong> LoRA: Low-Rank Adaptation of Large Language Models </strong> <code>ICLR 2022</code> <code>#LoRA</code> <br> <a href="https://arxiv.org/abs/2106.09685"><img src="https://img.shields.io/badge/arXiv-2106.09685-b31b1b.svg?style=flat-square"></a> <a href="https://github.com/microsoft/LoRA"><img src="https://img.shields.io/github/stars/microsoft/LoRA?style=flat-square&logo=github&logoColor=fff&labelColor=black"></a> </summary>

</details>

## Analysis

<details><summary> <strong> Towards a Unified View of Parameter-Efficient Transfer Learning </strong> <code>ICLR 2022</code> <br> <a href="https://arxiv.org/abs/2110.04366"><img src="https://img.shields.io/badge/arXiv-2110.04366-b31b1b.svg?style=flat-square"></a> <a href="https://github.com/jxhe/unify-parameter-efficient-tuning"><img src="https://img.shields.io/github/stars/jxhe/unify-parameter-efficient-tuning?style=flat-square&logo=github&logoColor=fff&labelColor=black"></a> </summary>

</details>

<details><summary> <strong> UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning </strong> <code>ACL 2022</code> <code>#UniPELT</code> <br> <a href="https://arxiv.org/abs/2110.07577"><img src="https://img.shields.io/badge/arXiv-2110.07577-b31b1b.svg?style=flat-square"></a> <a href="https://github.com/morningmoni/UniPELT"><img src="https://img.shields.io/github/stars/morningmoni/UniPELT?style=flat-square&logo=github&logoColor=fff&labelColor=black"></a> </summary>

</details>

<details><summary> <strong> Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models </strong> <code>Preprint</code> <br> <a href="https://arxiv.org/abs/2203.06904"><img src="https://img.shields.io/badge/arXiv-2203.06904-b31b1b.svg?style=flat-square"></a> <a href="https://github.com/thunlp/OpenDelta"><img src="https://img.shields.io/github/stars/thunlp/OpenDelta?style=flat-square&logo=github&logoColor=fff&labelColor=black"></a> </summary>

</details>

<details><summary> <strong> Parameter-Efficient Fine-Tuning Design Spaces </strong> <code>ICLR 2023</code> <code>#S4</code> <br> <a href="https://arxiv.org/abs/2301.01821"><img src="https://img.shields.io/badge/arXiv-2301.01821-b31b1b.svg?style=flat-square"></a> <a href="https://github.com/amazon-science/peft-design-spaces"><img src="https://img.shields.io/github/stars/amazon-science/peft-design-spaces?style=flat-square&logo=github&logoColor=fff&labelColor=black"></a> </summary>

</details>

<details><summary> <strong> Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning </strong> <code>Preprint</code> <br> <a href="https://arxiv.org/abs/2303.15647"><img src="https://img.shields.io/badge/arXiv-2303.15647-b31b1b.svg?style=flat-square"></a> </summary>

</details>

<details><summary> <strong> LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models </strong> <code>EMNLP 2023</code> <br> <a href="https://arxiv.org/abs/2304.01933"><img src="https://img.shields.io/badge/arXiv-2304.01933-b31b1b.svg?style=flat-square"></a> <a href="https://github.com/AGI-Edgerunners/LLM-Adapters"><img src="https://img.shields.io/github/stars/AGI-Edgerunners/LLM-Adapters?style=flat-square&logo=github&logoColor=fff&labelColor=black"></a> </summary>

</details>

<details><summary> <strong> A Comprehensive Analysis of Adapter Efficiency </strong> <code>ICML 2023</code> <br> <a href="https://arxiv.org/abs/2305.07491"><img src="https://img.shields.io/badge/arXiv-2305.07491-b31b1b.svg?style=flat-square"></a> <a href="https://github.com/AI4Bharat/adapter-efficiency"><img src="https://img.shields.io/github/stars/AI4Bharat/adapter-efficiency?style=flat-square&logo=github&logoColor=fff&labelColor=black"></a> </summary>

</details>

## Survey

<strong> Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey </strong> <br>
<a href="https://arxiv.org/abs/2403.14608"><img src="https://img.shields.io/badge/arXiv-2403.14608-b31b1b.svg?style=flat-square"></a>

## Useful Links
